{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aerial-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n",
      "Python 3.8.6\n",
      "1.7.1+cu110\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "!nvcc --version\n",
    "!python --version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "christian-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: cuda requested and available, will use gpu!\n"
     ]
    }
   ],
   "source": [
    "# Try to use GPU if available\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and not torch.cuda.is_available():\n",
    "    print(\"Error: cuda requested but not available, will use cpu instead!\")\n",
    "    device = torch.device('cpu')\n",
    "elif not use_cuda:\n",
    "    print(\"Info: will use cpu!\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(\"Info: cuda requested and available, will use gpu!\")\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legal-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, inputs, targets): \n",
    "        self.inputs  = inputs.astype(np.float32)\n",
    "        self.targets = targets.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-intent",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-interstate",
   "metadata": {},
   "source": [
    "We normalize all variables apart form the temperature. This includes y = RECO. For that reason we have to normalize the values that the neural network yields with the stats of the training data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reliable-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Synthetic4BookChap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "plain-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Q_10 = False\n",
    "mean = 1.5\n",
    "std = 0.2\n",
    "\n",
    "data = df[['SW_POT_sm', 'SW_POT_sm_diff', 'TA', 'Rb_syn', 'DateTime']]\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data_train = data[(data['DateTime'] > '2003-01-01') & (data['DateTime'] <= '2006-31-12')]\n",
    "X = data_train[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "X_mean = X.mean(axis=0)\n",
    "X_sd = X.std(axis=0)\n",
    "X_mean[2] = 0    # we want to keep the temperature untouched\n",
    "X_sd[2] = 1     # we want to keep the temperature untouched\n",
    "X = (X - X_mean)/ X_sd\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_train['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_train['Rb_syn'] * Q_10 ** (0.1 * (data_train['TA'] - 15))).values\n",
    "Y_mean = Y.mean()\n",
    "Y_sd = Y.std()\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "traindata = Dataset(X,Y)\n",
    "\n",
    "\n",
    "data_val = data[(data['DateTime'] > '2007-01-01') & (data['DateTime'] <= '2007-31-12')]\n",
    "X = data_val[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_val['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_val['Rb_syn'] * Q_10 ** (0.1 * (data_val['TA'] - 15))).values\n",
    "X = (X - X_mean)/ X_sd\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "valdata = Dataset(X,Y)\n",
    "\n",
    "\n",
    "data_test = data[(data['DateTime'] > '2008-01-01') & (data['DateTime'] <= '2008-31-12')]\n",
    "X = data_test[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_test['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_test['Rb_syn'] * Q_10 ** (0.1 * (data_test['TA'] - 15))).values\n",
    "X = (X - X_mean)/ X_sd\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "testdata = Dataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "aboriginal-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(traindata, batch_size=256, shuffle=True , drop_last=True) \n",
    "valLoader  = torch.utils.data.DataLoader(valdata, batch_size=256, shuffle=False, drop_last=True) \n",
    "testLoader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-private",
   "metadata": {},
   "source": [
    "# Deterministic Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-organ",
   "metadata": {},
   "source": [
    "# Defining the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "removable-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q10nn(nn.Module):\n",
    "    def __init__(self, Q10_init, y_mean, y_sd, include_TA=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.mode = 'generate'\n",
    "        input_dim = 2\n",
    "        hidden_layer = 2\n",
    "        hidden_nodes = 8\n",
    "        self.y_mean = y_mean\n",
    "        self.y_sd = y_sd\n",
    "        \n",
    "        self.Q = torch.nn.Parameter(torch.tensor(float(Q10_init)))\n",
    "        self.Q.requires_grad = True\n",
    "     \n",
    "        if hidden_layer == 0:\n",
    "            layers = [nn.Linear(input_dim, 1)]\n",
    "            layers.append(nn.Softplus())\n",
    "        else:\n",
    "            layers = [nn.Linear(input_dim, hidden_nodes)]\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            for i in range(hidden_layer-1):\n",
    "                layers.append(nn.Linear(hidden_nodes, hidden_nodes))\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "            layers.append(nn.Linear(hidden_nodes, 1))\n",
    "            layers.append(nn.Softplus())\n",
    "                                                            \n",
    "        self.lls = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.lls(x[:,:-1]).squeeze(-1) * torch.pow(self.Q, 0.1 * (x[:,-1]-15))\n",
    "        y = (y - self.y_mean) / self.y_sd\n",
    "        return y\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_TA = True\n",
    "model = q10nn(Y_mean, Y_sd, Q10_init, std_init, include_TA).to(device)\n",
    "model.apply(weights_init)\n",
    "print(model.mu)\n",
    "print(model.logvar.mul(0.5).exp())\n",
    "lr_init = 0.01\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam([{'params': model.lls.parameters(), 'lr': lr_init,'weight_decay': weight_decay},\n",
    "                                {'params': [model.mu, model.logvar], 'lr': lr_init, 'weight_decay': 0}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-mozambique",
   "metadata": {},
   "source": [
    "# Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "academic-operator",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'q10nn_VB' object has no attribute 'Q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-6111baa248e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mL2val_accum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlossL2val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mhistory_Q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mhistory_L2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mL2_accum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mhistory_L2val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mL2val_accum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalLoader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m    779\u001b[0m             type(self).__name__, name))\n\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'q10nn_VB' object has no attribute 'Q'"
     ]
    }
   ],
   "source": [
    "history_L2 = []\n",
    "history_L2val = []\n",
    "history_Q = []\n",
    "EPOCHS = 10\n",
    "model.to('cpu')\n",
    "\n",
    "criterionL2 = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Compute the initial Validation and Training loss\n",
    "model.eval()\n",
    "L2_accum = 0.0\n",
    "for i, traindata in enumerate(trainLoader, 0):\n",
    "    inputs_curr, targets_curr = traindata\n",
    "\n",
    "    outputs= model(inputs_curr)\n",
    "    outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "    lossL2 = criterionL2(outputs, targets_curr)\n",
    "    L2_accum += lossL2.item()\n",
    "\n",
    "L2val_accum = 0.0\n",
    "for i, validata in enumerate(valLoader, 0):\n",
    "    inputs_curr, targets_curr = validata\n",
    "\n",
    "    outputs= model(inputs_curr)\n",
    "    outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "    lossL2val = criterionL2(outputs, targets_curr)\n",
    "    L2val_accum += lossL2val.item()\n",
    "\n",
    "history_Q.append(model.Q.clone())\n",
    "history_L2.append( L2_accum / len(trainLoader) )\n",
    "history_L2val.append( L2val_accum / len(valLoader) )\n",
    "\n",
    "print( \"Epoch: {}, L2 train: {:7.5f}, L1 vali: {:7.5f}\".format(0, history_L2[-1], history_L2val[-1]) )\n",
    "\n",
    "# Now start the training process\n",
    "print(\"Training from scratch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    L2_accum = 0.0\n",
    "    for i, traindata in enumerate(trainLoader, 0):\n",
    "        inputs_curr, targets_curr = traindata\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        gen_out = model(inputs_curr)\n",
    "        \n",
    "        lossL2 = criterionL2(gen_out, targets_curr)\n",
    "        lossL2.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        L2_accum += lossL2.item()        \n",
    "\n",
    "  # validation\n",
    "    model.eval()\n",
    "    L2val_accum = 0.0\n",
    "    for i, validata in enumerate(valLoader, 0):\n",
    "        inputs_curr, targets_curr = validata\n",
    "        \n",
    "        outputs= model(inputs_curr)\n",
    "        outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "        lossL2val = criterionL2(outputs, targets_curr)\n",
    "        L2val_accum += lossL2val.item()\n",
    "    \n",
    "    history_Q.append(model.Q.clone())\n",
    "    \n",
    "    # data for graph plotting\n",
    "    history_L2.append( L2_accum / len(trainLoader) )\n",
    "    history_L2val.append( L2val_accum / len(valLoader) )\n",
    "\n",
    "    if epoch % 1 ==0:\n",
    "        print( \"Epoch: {}, L2 train: {:7.5f}, L1 vali: {:7.5f}\".format(epoch+1, history_L2[-1], history_L2val[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-traveler",
   "metadata": {},
   "source": [
    "# Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l2train = np.asarray(history_L2)\n",
    "l2vali  = np.asarray(history_L2val)\n",
    "Q = np.asarray(history_Q)\n",
    "\n",
    "fix, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q, 'b', label = 'Q10')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Q10')\n",
    "ax[0].set_title('Q10 training history')\n",
    "\n",
    "ax[1].plot(np.arange(l2train.shape[0]),l2train,'b',label='Training loss')\n",
    "ax[1].plot(np.arange(l2vali.shape[0] ),l2vali ,'g',label='Validation loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].set_title('MSE training history')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-midwest",
   "metadata": {},
   "source": [
    "# Nondeterministic Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "miniature-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q10nn_VB(nn.Module):\n",
    "    def __init__(self, y_mean, y_sd, Q10_init, std_init=0.1, include_TA=True):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.mode = 'generate'\n",
    "        input_dim = 2\n",
    "        hidden_layer = 2\n",
    "        hidden_nodes = 8\n",
    "        if include_TA:\n",
    "            input_dim += 1\n",
    "\n",
    "        self.include_TA=include_TA\n",
    "        self.y_mean = y_mean\n",
    "        self.y_sd = y_sd\n",
    "        \n",
    "        self.mu = torch.nn.Parameter(torch.tensor(float(Q10_init)))\n",
    "        self.mu.requires_grad = True\n",
    "        self.logvar = torch.nn.Parameter(torch.tensor(np.log(std_init**2)))\n",
    "        self.logvar.requires_grad = True\n",
    "        \n",
    "        if hidden_layer == 0:\n",
    "            layers = [nn.Linear(input_dim, 1)]\n",
    "            layers.append(nn.Softplus())\n",
    "        else:\n",
    "            layers = [nn.Linear(input_dim, hidden_nodes)]\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            for i in range(hidden_layer-1):\n",
    "                layers.append(nn.Linear(hidden_nodes, hidden_nodes))\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "            layers.append(nn.Linear(hidden_nodes, 1))\n",
    "            layers.append(nn.Softplus())\n",
    "                                                            \n",
    "        self.lls = nn.Sequential(*layers)\n",
    "    \n",
    "    def sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # Reparameterization\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.sample(self.mu, self.logvar)\n",
    "        if self.include_TA:\n",
    "            y = self.lls(x).squeeze(-1) * torch.pow(q, 0.1 * (x[:,-1]-15))\n",
    "        else:\n",
    "            y = self.lls(x[:,:-1]).squeeze(-1) * torch.pow(q, 0.1 * (x[:,-1]-15))\n",
    "        y = (y - self.y_mean) / self.y_sd\n",
    "        return y\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "thermal-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0\n",
    "Q10_init = 2.0\n",
    "std_init = 0.5\n",
    "Q10_prior = Q10_init\n",
    "logvar_prior = torch.tensor(np.log(std_init**2))\n",
    "\n",
    "def VB_loss(y_pred, y, mu, logvar):\n",
    "    # Reconstruction losses are summed over all elements and batch, normalize loss by batch size\n",
    "    mse_loss = F.mse_loss(y_pred, y)    \n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - logvar_prior - ((mu-Q10_prior).pow(2) + logvar.exp())/logvar_prior.exp())\n",
    "    total_loss = mse_loss + beta * kld_loss\n",
    "\n",
    "    return total_loss, mse_loss, kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-behavior",
   "metadata": {},
   "source": [
    "# Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-carroll",
   "metadata": {},
   "source": [
    "Note that we choose a learning rate for Q10 that is 10 times higher than the one of the other parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "hungry-meter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(2., device='cuda:0', requires_grad=True)\n",
      "tensor(0.5000, device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "include_TA = False\n",
    "model = q10nn_VB(Y_mean, Y_sd, Q10_init, std_init, include_TA).to(device)\n",
    "model.apply(weights_init)\n",
    "print(model.mu)\n",
    "print(model.logvar.mul(0.5).exp())\n",
    "lr_init = 0.01\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam([{'params': model.lls.parameters(), 'lr': lr_init,'weight_decay': weight_decay},\n",
    "                                {'params': [model.mu, model.logvar], 'lr': lr_init, 'weight_decay': 0}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "unique-forestry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total train: 1.77057, mse train: 7.10832, KLD train: 0.00000, total test: 0.41061, mse test: 1.64846, KLD test: 0.00000\n",
      "Q mean:  2.00, Q std:  0.50\n",
      "Training from scratch\n",
      "Epoch: 1, total train: 0.30888, mse train: 1.24006, KLD train: 0.11337, total test: 0.01682, mse test: 0.06754, KLD test: 0.07503\n",
      "Q mean:  1.7027, Q std:  0.3353\n",
      "Epoch: 2, total train: 0.08800, mse train: 0.35328, KLD train: 0.51818, total test: 0.00866, mse test: 0.03476, KLD test: 0.17685\n",
      "Q mean:  1.5708, Q std:  0.2424\n",
      "Epoch: 3, total train: 0.06068, mse train: 0.24360, KLD train: 0.86365, total test: 0.00731, mse test: 0.02933, KLD test: 0.23608\n",
      "Q mean:  1.5537, Q std:  0.1879\n",
      "Epoch: 4, total train: 0.04751, mse train: 0.19075, KLD train: 1.07950, total test: 0.00640, mse test: 0.02567, KLD test: 0.29339\n",
      "Q mean:  1.5270, Q std:  0.1531\n",
      "Epoch: 5, total train: 0.03942, mse train: 0.15825, KLD train: 1.26343, total test: 0.00800, mse test: 0.03211, KLD test: 0.33779\n",
      "Q mean:  1.5103, Q std:  0.1306\n",
      "Epoch: 6, total train: 0.03621, mse train: 0.14537, KLD train: 1.43374, total test: 0.00708, mse test: 0.02843, KLD test: 0.36807\n",
      "Q mean:  1.5170, Q std:  0.1132\n",
      "Epoch: 7, total train: 0.03363, mse train: 0.13503, KLD train: 1.57384, total test: 0.00615, mse test: 0.02471, KLD test: 0.40215\n",
      "Q mean:  1.5111, Q std:  0.0993\n",
      "Epoch: 8, total train: 0.03168, mse train: 0.12718, KLD train: 1.71135, total test: 0.00618, mse test: 0.02482, KLD test: 0.42828\n",
      "Q mean:  1.5130, Q std:  0.0887\n",
      "Epoch: 9, total train: 0.03019, mse train: 0.12122, KLD train: 1.80745, total test: 0.00605, mse test: 0.02427, KLD test: 0.46241\n",
      "Q mean:  1.4953, Q std:  0.0799\n",
      "Epoch: 10, total train: 0.02897, mse train: 0.11631, KLD train: 1.90005, total test: 0.00614, mse test: 0.02466, KLD test: 0.48153\n",
      "Q mean:  1.5015, Q std:  0.0729\n"
     ]
    }
   ],
   "source": [
    "history_total = []\n",
    "history_mse = []\n",
    "history_kld = []\n",
    "\n",
    "history_total_val = []\n",
    "history_mse_val = []\n",
    "history_kld_val = []\n",
    "\n",
    "history_Q = []\n",
    "history_std = []\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Compute the initial Validation and Training loss\n",
    "model.eval()\n",
    "total_accum = 0.0\n",
    "mse_accum = 0.0\n",
    "kld_accum = 0.0\n",
    "\n",
    "for i, traindata in enumerate(trainLoader, 0):\n",
    "    inputs_curr, targets_curr = traindata\n",
    "\n",
    "    outputs= model(inputs_curr.to(device))\n",
    "    total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "\n",
    "    total_accum += total_loss.item()\n",
    "    mse_accum += mse_loss.item()\n",
    "    kld_accum += kld_loss.item()\n",
    "    \n",
    "total_accum_val = 0.0\n",
    "mse_accum_val = 0.0\n",
    "kld_accum_val = 0.0\n",
    "\n",
    "for i, validata in enumerate(valLoader, 0):\n",
    "    inputs_curr, targets_curr = validata\n",
    "\n",
    "    outputs= model(inputs_curr.to(device))\n",
    "    \n",
    "    total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "    \n",
    "    total_accum_val += total_loss.item()\n",
    "    mse_accum_val += mse_loss.item()\n",
    "    kld_accum_val += kld_loss.item()\n",
    "\n",
    "history_Q.append(model.mu.clone())\n",
    "history_std.append(model.logvar.clone())\n",
    "\n",
    "history_total.append( total_accum / len(trainLoader) )\n",
    "history_mse.append( mse_accum / len(valLoader) )\n",
    "history_kld.append( kld_accum / len(trainLoader) )\n",
    "\n",
    "history_total_val.append( total_accum_val / len(trainLoader) )\n",
    "history_mse_val.append( mse_accum_val / len(valLoader) )\n",
    "history_kld_val.append( kld_accum_val / len(trainLoader) )\n",
    "\n",
    "print( \"Epoch: {}, total train: {:7.5f}, mse train: {:7.5f}, KLD train: {:7.5f}, total test: {:7.5f}, mse test: {:7.5f}, KLD test: {:7.5f}\".format(0, history_total[-1], history_mse[-1], history_kld[-1], history_total_val[-1], history_mse_val[-1], history_kld_val[-1]))\n",
    "print( f\"Q mean: {history_Q[-1]: .2f}, Q std: {torch.exp(1/2*history_std[-1]): .2f}\")\n",
    "\n",
    "# Now start the training process\n",
    "print(\"Training from scratch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    total_accum = 0.0\n",
    "    mse_accum = 0.0\n",
    "    kld_accum = 0.0\n",
    "\n",
    "    for i, traindata in enumerate(trainLoader, 0):\n",
    "        inputs_curr, targets_curr = traindata\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs_curr.to(device))\n",
    "        total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)    \n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_accum += total_loss.item()\n",
    "        mse_accum += mse_loss.item()\n",
    "        kld_accum += kld_loss.item()\n",
    "\n",
    "  # validation\n",
    "    model.eval()\n",
    "    total_accum_val = 0.0\n",
    "    mse_accum_val = 0.0\n",
    "    kld_accum_val = 0.0\n",
    "\n",
    "    for i, validata in enumerate(valLoader, 0):\n",
    "        inputs_curr, targets_curr = validata\n",
    "        \n",
    "        outputs= model(inputs_curr.to(device))\n",
    "        total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "\n",
    "        total_accum_val += total_loss.item()\n",
    "        mse_accum_val += mse_loss.item()\n",
    "        kld_accum_val += kld_loss.item()\n",
    "            \n",
    "   \n",
    "    history_Q.append(model.mu.clone())\n",
    "    history_std.append(model.logvar.clone())\n",
    "\n",
    "    history_total.append( total_accum / len(trainLoader) )\n",
    "    history_mse.append( mse_accum / len(valLoader) )\n",
    "    history_kld.append( kld_accum / len(trainLoader) )\n",
    "\n",
    "    history_total_val.append( total_accum_val / len(trainLoader) )\n",
    "    history_mse_val.append( mse_accum_val / len(valLoader) )\n",
    "    history_kld_val.append( kld_accum_val / len(trainLoader) )\n",
    "\n",
    "    if epoch % 1 ==0:\n",
    "        print( \"Epoch: {}, total train: {:7.5f}, mse train: {:7.5f}, KLD train: {:7.5f}, total test: {:7.5f}, mse test: {:7.5f}, KLD test: {:7.5f}\".format(epoch+1, history_total[-1], history_mse[-1], history_kld[-1], history_total_val[-1], history_mse_val[-1], history_kld_val[-1]))\n",
    "        print( f\"Q mean: {history_Q[-1]: .4f}, Q std: {torch.exp(1/2*history_std[-1]): .4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-alarm",
   "metadata": {},
   "source": [
    "# Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "totaltrain = np.asarray(history_total)\n",
    "totalvali  = np.asarray(history_total_val)\n",
    "msetrain = np.asarray(history_mse)\n",
    "msevali  = np.asarray(history_mse_val)\n",
    "kldtrain = np.asarray(history_kld)\n",
    "kldvali  = np.asarray(history_kld_val)\n",
    "Q = np.asarray(history_Q)\n",
    "std = np.asarray(history_std)\n",
    "\n",
    "fix, ax = plt.subplots(1,5, figsize = (25,5))\n",
    "\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q, 'b', label = 'Q10')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Q10')\n",
    "ax[0].set_title('Q10 training history')\n",
    "\n",
    "ax[1].plot(np.arange(std.shape[0]), np.exp(1/2*std), 'b', label = 'std')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('std')\n",
    "ax[1].set_title('std training history')\n",
    "\n",
    "ax[2].plot(np.arange(totaltrain.shape[0]),totaltrain,'b',label='Training loss')\n",
    "ax[2].plot(np.arange(totalvali.shape[0] ),totalvali ,'g',label='Validation loss')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Total loss')\n",
    "ax[2].set_title('Total loss training history')\n",
    "\n",
    "ax[3].plot(np.arange(msetrain.shape[0]),msetrain,'b',label='Training loss')\n",
    "ax[3].plot(np.arange(msevali.shape[0] ),msevali ,'g',label='Validation loss')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('MSE loss')\n",
    "ax[3].set_title('MSE training history')\n",
    "\n",
    "ax[4].plot(np.arange(kldtrain.shape[0]),kldtrain,'b',label='Training loss')\n",
    "ax[4].plot(np.arange(kldvali.shape[0] ),kldvali ,'g',label='Validation loss')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('kld loss')\n",
    "ax[4].set_title('kld training history')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-wagon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
