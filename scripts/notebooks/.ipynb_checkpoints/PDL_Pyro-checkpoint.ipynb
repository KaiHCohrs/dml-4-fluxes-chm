{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "young-avenue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n",
      "Python 3.8.13\n",
      "1.11.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from pathlib import Path\n",
    "\n",
    "!nvcc --version\n",
    "!python --version\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "departmental-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "import torch.nn as nn\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.infer import SVI, Trace_ELBO, Predictive, TraceMeanField_ELBO\n",
    "from pyro.optim import ExponentialLR\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scheduled-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: cuda requested and available, will use gpu!\n"
     ]
    }
   ],
   "source": [
    "# Try to use GPU if available\n",
    "use_cuda = True\n",
    "\n",
    "if use_cuda and not torch.cuda.is_available():\n",
    "    print(\"Error: cuda requested but not available, will use cpu instead!\")\n",
    "    device = torch.device('cpu')\n",
    "elif not use_cuda:\n",
    "    print(\"Info: will use cpu!\")\n",
    "    device = torch.device('cpu')\n",
    "else:\n",
    "    print(\"Info: cuda requested and available, will use gpu!\")\n",
    "    device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accessible-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, inputs, targets): \n",
    "        self.inputs  = inputs.astype(np.float32)\n",
    "        self.targets = targets.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-chocolate",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-consolidation",
   "metadata": {},
   "source": [
    "We normalize all variables apart form the temperature. This includes y = RECO. For that reason we have to normalize the values that the neural network yields with the stats of the training data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "european-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Synthetic4BookChap.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "diagnostic-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Q_10 = False\n",
    "mean = 1.5\n",
    "std = 0.2\n",
    "\n",
    "data = df[['SW_POT_sm', 'SW_POT_sm_diff', 'TA', 'Rb_syn', 'DateTime']]\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data_train = data[(data['DateTime'] > '2003-01-01') & (data['DateTime'] <= '2006-31-12')]\n",
    "X = data_train[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "X_mean = X.mean(axis=0)\n",
    "X_sd = X.std(axis=0)\n",
    "X_mean[2] = 0    # we want to keep the temperature untouched\n",
    "X_sd[2] = 1     # we want to keep the temperature untouched\n",
    "X = (X - X_mean)/ X_sd\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_train['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_train['Rb_syn'] * Q_10 ** (0.1 * (data_train['TA'] - 15))).values\n",
    "Y_mean = Y.mean()\n",
    "Y_sd = Y.std()\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "traindata = Dataset(X,Y)\n",
    "\n",
    "\n",
    "data_val = data[(data['DateTime'] > '2007-01-01') & (data['DateTime'] <= '2007-31-12')]\n",
    "X = data_val[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_val['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_val['Rb_syn'] * Q_10 ** (0.1 * (data_val['TA'] - 15))).values\n",
    "X = (X - X_mean)/ X_sd\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "valdata = Dataset(X,Y)\n",
    "\n",
    "\n",
    "data_test = data[(data['DateTime'] > '2008-01-01') & (data['DateTime'] <= '2008-31-12')]\n",
    "X = data_test[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_test['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_test['Rb_syn'] * Q_10 ** (0.1 * (data_test['TA'] - 15))).values\n",
    "X = (X - X_mean)/ X_sd\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "testdata = Dataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "stainless-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(traindata, batch_size=256, shuffle=True , drop_last=True) \n",
    "valLoader  = torch.utils.data.DataLoader(valdata, batch_size=256, shuffle=False, drop_last=True) \n",
    "testLoader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-samuel",
   "metadata": {},
   "source": [
    "# Defining the network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-banks",
   "metadata": {},
   "source": [
    "First I want to define a normal network in such a way that I can do inference on Q10 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "informal-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data[(data['DateTime'] > '2003-01-01') & (data['DateTime'] <= '2003-31-12')]\n",
    "X = data_train[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "X_mean = X.mean(axis=0)\n",
    "X_sd = X.std(axis=0)\n",
    "X_mean[2] = 0    # we want to keep the temperature untouched\n",
    "X_sd[2] = 1     # we want to keep the temperature untouched\n",
    "X = (X - X_mean)/ X_sd\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_train['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_train['Rb_syn'] * Q_10 ** (0.1 * (data_train['TA'] - 15))).values\n",
    "Y_mean = Y.mean()\n",
    "Y_sd = Y.std()\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "traindata = Dataset(X,Y)\n",
    "\n",
    "\n",
    "data_test = data[(data['DateTime'] > '2004-01-01') & (data['DateTime'] <= '2004-31-12')]\n",
    "X = data_test[['SW_POT_sm', 'SW_POT_sm_diff', 'TA']].values\n",
    "if sample_Q_10:\n",
    "    Q_10 = np.random.normal(mean, std, len(data_test['Rb_syn']))\n",
    "else:\n",
    "    Q_10 = 1.5\n",
    "Y = (data_test['Rb_syn'] * Q_10 ** (0.1 * (data_test['TA'] - 15))).values\n",
    "X = (X - X_mean)/ X_sd\n",
    "Y = (Y - Y_mean)/ Y_sd\n",
    "testdata = Dataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ranking-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(traindata, batch_size=256, shuffle=True , drop_last=True) \n",
    "#valLoader  = torch.utils.data.DataLoader(valdata, batch_size=256, shuffle=False, drop_last=True) \n",
    "testLoader = torch.utils.data.DataLoader(testdata, batch_size=256, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "attractive-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rb(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_nodes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_nodes)\n",
    "        self.fc2 = nn.Linear(hidden_nodes, hidden_nodes)\n",
    "        self.fc3 = nn.Linear(hidden_nodes, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.softplus(self.fc3(x))\n",
    "    \n",
    "class q10_module(nn.Module):\n",
    "    def __init__(self, mean, sd):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.sd = sd\n",
    "        \n",
    "    def forward(self, x, t, q10):\n",
    "        print\n",
    "        y =  torch.multiply(x, torch.pow(q10, (0.1*(t - 15))))\n",
    "        return (y-self.mean) / self.sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "false-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q10_bayes(nn.Module):\n",
    "    def __init__(self, q10_prior_mean, q10_prior_sd, q10_post_mean, q10_post_sd, y_mean, y_sd, input_dim=2, hidden_nodes=10, mean=0, sd=1, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.q10_module = q10_module(y_mean, y_sd)\n",
    "        self.Rb = Rb(input_dim, hidden_nodes)\n",
    "        if use_cuda:\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.q10_prior_mean=q10_prior_mean\n",
    "        self.q10_prior_sd=q10_prior_sd\n",
    "        self.q10_post_mean=q10_post_mean\n",
    "        self.q10_post_sd=q10_post_sd\n",
    "        self.T=10\n",
    "    \n",
    "    def model(self, x, y=None):\n",
    "        pyro.module(\"q10_module\", self.q10_module)\n",
    "        pyro.module(\"Rb\", self.Rb)\n",
    "        q_mean = pyro.param(\"q_mean\", lambda: torch.tensor([self.q10_prior_mean]))\n",
    "        q_sd = pyro.param(\"q_sd\", lambda: torch.tensor([self.q10_prior_sd]), constraint=constraints.positive)\n",
    "        \n",
    "        #sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        sigma = dist.Uniform(0., 10.).sample()\n",
    "        x, t = x[:, :-1], x[:,-1]\n",
    "        x = self.Rb(x).squeeze()\n",
    "        \n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            # sample from the prior\n",
    "            with pyro.poutine.scale(scale=self.T):\n",
    "                q = pyro.sample(\"q10\", dist.Normal(q_mean, q_sd))\n",
    "                reco = self.q10_module(x, t, q)\n",
    "                pyro.sample(\"obs\", dist.Normal(reco, sigma), obs=y)\n",
    "\n",
    "    def guide(self, x, y=None):\n",
    "        q_mean = pyro.param(\"q_mean\", lambda: torch.tensor([self.q10_post_mean]))\n",
    "        q_sd = pyro.param(\"q_sd\", lambda: torch.tensor([self.q10_post_sd]), constraint=constraints.positive)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            q = pyro.sample(\"q10\", dist.Normal(q_mean, q_sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "equivalent-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "q10 = Q10_bayes(1.5, 5.0, 1.5, 0.2, Y_mean, Y_sd, use_cuda=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "electoral-nirvana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220315.2325)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"344pt\" height=\"349pt\"\n",
       " viewBox=\"0.00 0.00 344.00 349.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 345)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-345 340,-345 340,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_data</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"13,-8 13,-155 83,-155 83,-8 13,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- q_mean -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>q_mean</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"44,-269.5 0,-269.5 0,-254.5 44,-254.5 44,-269.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"22\" y=\"-258.3\" font-family=\"Times,serif\" font-size=\"14.00\">q_mean</text>\n",
       "</g>\n",
       "<!-- q10 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>q10</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"48\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">q10</text>\n",
       "</g>\n",
       "<!-- q_mean&#45;&gt;q10 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>q_mean&#45;&gt;q10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M23.33,-254.32C26.82,-236.69 36.46,-188.13 42.61,-157.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"46.1,-157.53 44.62,-147.04 39.24,-156.17 46.1,-157.53\"/>\n",
       "</g>\n",
       "<!-- q_sd -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>q_sd</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"88,-269.5 62,-269.5 62,-254.5 88,-254.5 88,-269.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"75\" y=\"-258.3\" font-family=\"Times,serif\" font-size=\"14.00\">q_sd</text>\n",
       "</g>\n",
       "<!-- q_sd&#45;&gt;q10 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>q_sd&#45;&gt;q10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73.62,-254.32C69.99,-236.69 59.98,-188.13 53.6,-157.17\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"56.96,-156.13 51.51,-147.04 50.1,-157.54 56.96,-156.13\"/>\n",
       "</g>\n",
       "<!-- obs -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>obs</title>\n",
       "<ellipse fill=\"grey\" stroke=\"black\" cx=\"48\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">obs</text>\n",
       "</g>\n",
       "<!-- q10&#45;&gt;obs -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>q10&#45;&gt;obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48,-110.7C48,-102.98 48,-93.71 48,-85.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"51.5,-85.1 48,-75.1 44.5,-85.1 51.5,-85.1\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-325.8\" font-family=\"Times,serif\" font-size=\"14.00\">q10 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-310.8\" font-family=\"Times,serif\" font-size=\"14.00\">obs ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-295.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc1.weight ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-280.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc1.bias ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-265.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc2.weight ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-250.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc2.bias ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc3.weight ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">Rb$$$fc3.bias ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_mean ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-190.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_sd ∈ GreaterThan(lower_bound=0.0)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f21107559a0>"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.render_model(q10.model, model_args=(torch.tensor(X, dtype=torch.float), torch.tensor(Y, dtype = torch.float)), render_distributions=True, render_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "athletic-messaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 3.0.0 (20220315.2325)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"344pt\" height=\"172pt\"\n",
       " viewBox=\"0.00 0.00 344.00 172.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 168)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-168 340,-168 340,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_data</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"13,-8 13,-83 83,-83 83,-8 13,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"63.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- q_mean -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>q_mean</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"44,-145 0,-145 0,-130 44,-130 44,-145\"/>\n",
       "<text text-anchor=\"middle\" x=\"22\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_mean</text>\n",
       "</g>\n",
       "<!-- q10 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>q10</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"48\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"48\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">q10</text>\n",
       "</g>\n",
       "<!-- q_mean&#45;&gt;q10 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>q_mean&#45;&gt;q10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M24.22,-129.81C27.53,-119.8 33.96,-100.38 39.3,-84.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"42.63,-85.35 42.45,-74.76 35.98,-83.15 42.63,-85.35\"/>\n",
       "</g>\n",
       "<!-- q_sd -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>q_sd</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"88,-145 62,-145 62,-130 88,-130 88,-145\"/>\n",
       "<text text-anchor=\"middle\" x=\"75\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_sd</text>\n",
       "</g>\n",
       "<!-- q_sd&#45;&gt;q10 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>q_sd&#45;&gt;q10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.7,-129.81C69.26,-119.8 62.58,-100.38 57.03,-84.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"60.33,-83.08 53.76,-74.76 53.71,-85.35 60.33,-83.08\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\">q10 ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_mean ∈ Real()</text>\n",
       "<text text-anchor=\"start\" x=\"114\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\">q_sd ∈ GreaterThan(lower_bound=0.0)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f21106b83a0>"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.render_model(q10.guide, model_args=(torch.tensor(X, dtype=torch.float),), render_distributions=True, render_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "attached-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "    # do a training epoch over each mini-batch x returned\n",
    "    # by the data loader\n",
    "    for x, y in train_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        # do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x,y)\n",
    "\n",
    "    # return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train\n",
    "\n",
    "def evaluate(svi, test_loader, use_cuda=False):\n",
    "    # initialize loss accumulator\n",
    "    test_loss = 0.\n",
    "    # compute the loss over the entire test set\n",
    "    for x, y in test_loader:\n",
    "        # if on GPU put mini-batch into CUDA memory\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        # compute ELBO estimate and accumulate loss\n",
    "        test_loss += svi.evaluate_loss(x, y)\n",
    "    normalizer_test = len(test_loader.dataset)\n",
    "    total_epoch_loss_test = test_loss / normalizer_test\n",
    "    return total_epoch_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "breeding-cemetery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n",
      "1.2914016299999716e-11\n"
     ]
    }
   ],
   "source": [
    "for parameter in optimizer.optim_objs.values():\n",
    "    print(parameter.param_groups[0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "trying-allah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    model_args,\n",
    "    model_path,\n",
    "    device,\n",
    "    dataloaders,\n",
    "    dataset_sizes,\n",
    "    scheduler_args,\n",
    "    num_epochs=100,\n",
    "    early_stop_patience=10,\n",
    "    temperature=10,\n",
    "    decay_steps = 5,\n",
    "    ):\n",
    "\n",
    "    # clear param store\n",
    "    pyro.clear_param_store()\n",
    "    \n",
    "    model = model(**model_args)\n",
    "    model.to(device)\n",
    "    #optimizer = pyro.optim.ClippedAdam(optimizer_args)\n",
    "    optimizer = ExponentialLR(scheduler_args)\n",
    "    svi = SVI(model.model, model.guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "    best_loss = np.inf\n",
    "    early_stop_count = 0\n",
    "    decay_steps_count = 0\n",
    "    Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            running_loss = 0.0\n",
    "            num_preds = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            bar = tqdm(\n",
    "                dataloaders[phase],\n",
    "                desc=\"Epoch {} {}\".format(epoch, phase).ljust(20),\n",
    "            )\n",
    "            for i, batch in enumerate(bar):\n",
    "                inputs = batch[0].to(device)\n",
    "                outputs = batch[1].to(device)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss = svi.step(inputs, outputs)\n",
    "                else:\n",
    "                    loss = svi.evaluate_loss(inputs, outputs)\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss / inputs.size(0)\n",
    "                num_preds += 1\n",
    "                if i % 10 == 0:\n",
    "                    bar.set_postfix(\n",
    "                        loss=\"{:.2f}\".format(running_loss / num_preds),\n",
    "                        early_stop_count=early_stop_count,\n",
    "                    )\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            # deep copy the model\n",
    "            if phase == \"val\":\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    early_stop_count = 0\n",
    "                else:\n",
    "                    early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= early_stop_patience:\n",
    "            optimizer.step()\n",
    "            decay_steps_count += 1\n",
    "            early_stop_count = 0\n",
    "            print(f\"{decay_steps_count} decay step\")\n",
    "        \n",
    "        if decay_steps_count >= decay_steps:\n",
    "            break\n",
    "\n",
    "    # Save model weights\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "absent-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.optim import MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "renewable-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'train': trainLoader, 'val': testLoader}\n",
    "dataset_sizes = {'train': len(trainLoader), 'val': len(testLoader)}\n",
    "\n",
    "model_args = dict( q10_prior_mean=1.5, q10_prior_sd=1.0, q10_post_mean=2.0, q10_post_sd=0.2, y_mean=Y_mean, y_sd=Y_sd, input_dim=2, hidden_nodes=10, mean=0, sd=1, use_cuda=False)\n",
    "model_path = '/usr/users/kcohrs/bayesian-q10/exp/test/state'\n",
    "\n",
    "scheduler = ExponentialLR({'optimizer': Adam,\n",
    "                             'optim_args': {'lr': 0.1, 'weight_decay': 0.0},\n",
    "                             'clip_args': {'clip_norm': 0.5},\n",
    "                             'gamma': 0.3})\n",
    "scheduler = MultiStepLR({'optimizer': Adam,\n",
    "                             'optim_args': {'lr': 0.1},\n",
    "                             'milestones': [20],\n",
    "                             'gamma': 0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "advised-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 train       :   0%|          | 0/68 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-464-9f40c12beeca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = train(Q10_bayes,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-449-950fb481391c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_args, model_path, device, dataloaders, dataset_sizes, scheduler, num_epochs, early_stop_patience, temperature, decay_steps)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# actually perform gradient steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# zero gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValuesView\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     def _get_optim(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/optim.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_objs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# create a single optim object for that param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m                 \u001b[0;31m# create a gradient clipping function if specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_grad_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m_get_optim\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     ):\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt_scheduler_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/optim.py\u001b[0m in \u001b[0;36m_get_optim\u001b[0;34m(self, param)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_optim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt_optim_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_optim_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# helper to fetch the optim args if callable (only used internally)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'lr'"
     ]
    }
   ],
   "source": [
    "model = train(Q10_bayes,\n",
    "    model_args = model_args,\n",
    "    model_path = model_path,\n",
    "    device = 'cpu',\n",
    "    dataloaders = dataloaders,\n",
    "    dataset_sizes = dataset_sizes,\n",
    "    scheduler = scheduler,\n",
    "    num_epochs=100,\n",
    "    early_stop_patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "consistent-mapping",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "optim_args must be function that returns defaults or a defaults dictionary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-455-bf3747605c05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/pytorch_optimizers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(optim_args, clip_args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     _PyroOptim = (\n\u001b[0;32m---> 23\u001b[0;31m         lambda _Optim: lambda optim_args, clip_args=None: PyroOptim(\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0m_Optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyro/optim/optim.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optim_constructor, optim_args, clip_args)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# must be callable or dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         assert callable(optim_args) or isinstance(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0moptim_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         ), \"optim_args must be function that returns defaults or a defaults dictionary\"\n",
      "\u001b[0;31mAssertionError\u001b[0m: optim_args must be function that returns defaults or a defaults dictionary"
     ]
    }
   ],
   "source": [
    "Adam([{'params': model.parameters(), 'lr': 0.1,'weight_decay': 0}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ordinary-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 000]  average training loss: 10444.8701\n",
      "q_mean [1.8471924]\n",
      "q_sd [0.22889851]\n",
      "[epoch 000] average test loss: 77.1143\n",
      "[epoch 001]  average training loss: 47.3727\n",
      "q_mean [1.7226673]\n",
      "q_sd [0.25274506]\n",
      "[epoch 002]  average training loss: 41.0665\n",
      "q_mean [1.6846907]\n",
      "q_sd [0.2379938]\n",
      "[epoch 002] average test loss: 23.4941\n",
      "[epoch 003]  average training loss: 22.5029\n",
      "q_mean [1.6736076]\n",
      "q_sd [0.21911663]\n",
      "[epoch 004]  average training loss: 26.8865\n",
      "q_mean [1.670322]\n",
      "q_sd [0.20416711]\n",
      "[epoch 004] average test loss: 23.1591\n",
      "[epoch 005]  average training loss: 24.1624\n",
      "q_mean [1.660793]\n",
      "q_sd [0.19291064]\n",
      "[epoch 006]  average training loss: 25.1029\n",
      "q_mean [1.6605445]\n",
      "q_sd [0.18434379]\n",
      "[epoch 006] average test loss: 23.5485\n",
      "[epoch 007]  average training loss: 22.0332\n",
      "q_mean [1.652406]\n",
      "q_sd [0.17758122]\n",
      "[epoch 008]  average training loss: 21.9467\n",
      "q_mean [1.6438129]\n",
      "q_sd [0.17227161]\n",
      "[epoch 008] average test loss: 36.5589\n",
      "[epoch 009]  average training loss: 26.9616\n",
      "q_mean [1.635902]\n",
      "q_sd [0.16816913]\n",
      "[epoch 010]  average training loss: 48.0468\n",
      "q_mean [1.6315632]\n",
      "q_sd [0.16483875]\n",
      "[epoch 010] average test loss: 26220.1141\n",
      "[epoch 011]  average training loss: 69.8208\n",
      "q_mean [1.6260133]\n",
      "q_sd [0.16219117]\n",
      "[epoch 012]  average training loss: 95.4605\n",
      "q_mean [1.6248055]\n",
      "q_sd [0.16007178]\n",
      "[epoch 012] average test loss: 30.0967\n",
      "[epoch 013]  average training loss: 24.6547\n",
      "q_mean [1.6218853]\n",
      "q_sd [0.15838054]\n",
      "[epoch 014]  average training loss: 28.3315\n",
      "q_mean [1.6192341]\n",
      "q_sd [0.15699899]\n",
      "[epoch 014] average test loss: 1445.8478\n",
      "[epoch 015]  average training loss: 22.1543\n",
      "q_mean [1.6192466]\n",
      "q_sd [0.1558826]\n",
      "[epoch 016]  average training loss: 68.7728\n",
      "q_mean [1.6172907]\n",
      "q_sd [0.15501393]\n",
      "[epoch 016] average test loss: 25.7569\n",
      "[epoch 017]  average training loss: 27.6182\n",
      "q_mean [1.6166412]\n",
      "q_sd [0.15428749]\n",
      "[epoch 018]  average training loss: 23.1426\n",
      "q_mean [1.6163316]\n",
      "q_sd [0.15371534]\n",
      "[epoch 018] average test loss: 2526.5407\n",
      "[epoch 019]  average training loss: 40.4882\n",
      "q_mean [1.6160566]\n",
      "q_sd [0.15324342]\n",
      "[epoch 020]  average training loss: 83.9852\n",
      "q_mean [1.6158768]\n",
      "q_sd [0.1528629]\n",
      "[epoch 020] average test loss: 22.4816\n",
      "[epoch 021]  average training loss: 80.6791\n",
      "q_mean [1.6154314]\n",
      "q_sd [0.15255177]\n",
      "[epoch 022]  average training loss: 22.5700\n",
      "q_mean [1.6150067]\n",
      "q_sd [0.15229502]\n",
      "[epoch 022] average test loss: 22.1911\n",
      "[epoch 023]  average training loss: 23.1038\n",
      "q_mean [1.6147249]\n",
      "q_sd [0.15209086]\n",
      "[epoch 024]  average training loss: 22.2834\n",
      "q_mean [1.614599]\n",
      "q_sd [0.15192336]\n",
      "[epoch 024] average test loss: 22.8632\n",
      "[epoch 025]  average training loss: 725.2844\n",
      "q_mean [1.6143361]\n",
      "q_sd [0.1517878]\n",
      "[epoch 026]  average training loss: 23.6050\n",
      "q_mean [1.6142505]\n",
      "q_sd [0.15168165]\n",
      "[epoch 026] average test loss: 22.9850\n",
      "[epoch 027]  average training loss: 22.2769\n",
      "q_mean [1.6141814]\n",
      "q_sd [0.15159348]\n",
      "[epoch 028]  average training loss: 24.2729\n",
      "q_mean [1.6141325]\n",
      "q_sd [0.15152366]\n",
      "[epoch 028] average test loss: 23.7520\n",
      "[epoch 029]  average training loss: 22.8142\n",
      "q_mean [1.6140007]\n",
      "q_sd [0.15146294]\n",
      "[epoch 030]  average training loss: 25.8241\n",
      "q_mean [1.6138436]\n",
      "q_sd [0.15141217]\n",
      "[epoch 030] average test loss: 22.2512\n",
      "[epoch 031]  average training loss: 24.4469\n",
      "q_mean [1.6138171]\n",
      "q_sd [0.15137173]\n",
      "[epoch 032]  average training loss: 92.1058\n",
      "q_mean [1.6138338]\n",
      "q_sd [0.15133934]\n",
      "[epoch 032] average test loss: 25.7684\n",
      "[epoch 033]  average training loss: 22.6499\n",
      "q_mean [1.613763]\n",
      "q_sd [0.1513132]\n",
      "[epoch 034]  average training loss: 39.0461\n",
      "q_mean [1.6137224]\n",
      "q_sd [0.15129155]\n",
      "[epoch 034] average test loss: 22.9205\n",
      "[epoch 035]  average training loss: 25.3519\n",
      "q_mean [1.613707]\n",
      "q_sd [0.15127376]\n",
      "[epoch 036]  average training loss: 24.3603\n",
      "q_mean [1.6136929]\n",
      "q_sd [0.15125902]\n",
      "[epoch 036] average test loss: 22.1783\n",
      "[epoch 037]  average training loss: 25.7020\n",
      "q_mean [1.6137053]\n",
      "q_sd [0.15124713]\n",
      "[epoch 038]  average training loss: 22.8816\n",
      "q_mean [1.6136926]\n",
      "q_sd [0.15123738]\n",
      "[epoch 038] average test loss: 22.2283\n",
      "[epoch 039]  average training loss: 64.9586\n",
      "q_mean [1.6136729]\n",
      "q_sd [0.15122929]\n",
      "[epoch 040]  average training loss: 23.0025\n",
      "q_mean [1.613662]\n",
      "q_sd [0.1512229]\n",
      "[epoch 040] average test loss: 23.1710\n",
      "[epoch 041]  average training loss: 29.0706\n",
      "q_mean [1.613653]\n",
      "q_sd [0.1512178]\n",
      "[epoch 042]  average training loss: 207.0645\n",
      "q_mean [1.6136398]\n",
      "q_sd [0.1512137]\n",
      "[epoch 042] average test loss: 22.9018\n",
      "[epoch 043]  average training loss: 22.4216\n",
      "q_mean [1.6136286]\n",
      "q_sd [0.15121037]\n",
      "[epoch 044]  average training loss: 21.6387\n",
      "q_mean [1.6136225]\n",
      "q_sd [0.15120754]\n",
      "[epoch 044] average test loss: 28.2231\n",
      "[epoch 045]  average training loss: 23.8024\n",
      "q_mean [1.6136177]\n",
      "q_sd [0.1512052]\n",
      "[epoch 046]  average training loss: 21.3846\n",
      "q_mean [1.6136129]\n",
      "q_sd [0.15120327]\n",
      "[epoch 046] average test loss: 35.2548\n",
      "[epoch 047]  average training loss: 22.8477\n",
      "q_mean [1.6136098]\n",
      "q_sd [0.1512017]\n",
      "[epoch 048]  average training loss: 24.7944\n",
      "q_mean [1.6136061]\n",
      "q_sd [0.15120041]\n",
      "[epoch 048] average test loss: 10384.3079\n",
      "[epoch 049]  average training loss: 22.6219\n",
      "q_mean [1.6136034]\n",
      "q_sd [0.1511994]\n",
      "[epoch 050]  average training loss: 22.3633\n",
      "q_mean [1.6136013]\n",
      "q_sd [0.15119852]\n",
      "[epoch 050] average test loss: 22.6789\n",
      "[epoch 051]  average training loss: 23.7698\n",
      "q_mean [1.6135997]\n",
      "q_sd [0.15119791]\n",
      "[epoch 052]  average training loss: 36.8377\n",
      "q_mean [1.6135979]\n",
      "q_sd [0.1511973]\n",
      "[epoch 052] average test loss: 72.6483\n",
      "[epoch 053]  average training loss: 21.7404\n",
      "q_mean [1.6135974]\n",
      "q_sd [0.15119682]\n",
      "[epoch 054]  average training loss: 22.7679\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119652]\n",
      "[epoch 054] average test loss: 22.8836\n",
      "[epoch 055]  average training loss: 24.2512\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119621]\n",
      "[epoch 056]  average training loss: 24.7796\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119591]\n",
      "[epoch 056] average test loss: 25.2965\n",
      "[epoch 057]  average training loss: 22.5190\n",
      "q_mean [1.613597]\n",
      "q_sd [0.1511956]\n",
      "[epoch 058]  average training loss: 22.9871\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 058] average test loss: 24.9483\n",
      "[epoch 059]  average training loss: 26.4211\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 060]  average training loss: 15785315.2363\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 060] average test loss: 22.2569\n",
      "[epoch 061]  average training loss: 22.2530\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 062]  average training loss: 46.9860\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 062] average test loss: 22.4385\n",
      "[epoch 063]  average training loss: 23.7392\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 064]  average training loss: 23.7097\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 064] average test loss: 23.6939\n",
      "[epoch 065]  average training loss: 181.8905\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 066]  average training loss: 22.9337\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 066] average test loss: 28.7713\n",
      "[epoch 067]  average training loss: 22.6235\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 068]  average training loss: 22.1559\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 068] average test loss: 23.8860\n",
      "[epoch 069]  average training loss: 24.8876\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 070]  average training loss: 23.5884\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 070] average test loss: 25.4375\n",
      "[epoch 071]  average training loss: 25.2875\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 072]  average training loss: 26.2825\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 072] average test loss: 22.0239\n",
      "[epoch 073]  average training loss: 22.7880\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 074]  average training loss: 22.5279\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 074] average test loss: 22.3833\n",
      "[epoch 075]  average training loss: 23.8792\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 076]  average training loss: 23.0021\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 076] average test loss: 25.3948\n",
      "[epoch 077]  average training loss: 21.1429\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 078]  average training loss: 2161.6660\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 078] average test loss: 23.2037\n",
      "[epoch 079]  average training loss: 22.4981\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 080]  average training loss: 23.4840\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 080] average test loss: 22.8451\n",
      "[epoch 081]  average training loss: 22.4201\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 082]  average training loss: 21.5267\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 082] average test loss: 74.0181\n",
      "[epoch 083]  average training loss: 27.9577\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 084]  average training loss: 103.8974\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 084] average test loss: 23.4050\n",
      "[epoch 085]  average training loss: 25.5565\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 086]  average training loss: 23.6046\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 086] average test loss: 21.4021\n",
      "[epoch 087]  average training loss: 22.1285\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 088]  average training loss: 21.5459\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 088] average test loss: 22.2839\n",
      "[epoch 089]  average training loss: 22.8395\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 090]  average training loss: 22.9821\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 090] average test loss: 40.1333\n",
      "[epoch 091]  average training loss: 23.5506\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 092]  average training loss: 22.6873\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 092] average test loss: 22.0784\n",
      "[epoch 093]  average training loss: 23.2769\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 094]  average training loss: 23.0720\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 094] average test loss: 558.3792\n",
      "[epoch 095]  average training loss: 22.7053\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 096]  average training loss: 23.7787\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 096] average test loss: 22.2231\n",
      "[epoch 097]  average training loss: 21.3950\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 098]  average training loss: 28.3067\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n",
      "[epoch 098] average test loss: 23.5413\n",
      "[epoch 099]  average training loss: 26.6177\n",
      "q_mean [1.613597]\n",
      "q_sd [0.15119548]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = trainLoader, testLoader\n",
    "USE_CUDA = False\n",
    "LEARNING_RATE = 1.0e-2\n",
    "NUM_EPOCHS = 100\n",
    "TEST_FREQUENCY = 2\n",
    "\n",
    "# clear param store\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup the VAE\n",
    "q10 = Q10_bayes(1.5, 1.0, 2.0, 0.2, Y_mean, Y_sd, use_cuda=USE_CUDA)\n",
    "\n",
    "# setup the optimizer\n",
    "#adam_args = {\"lr\": LEARNING_RATE}\n",
    "#optimizer = Adam(adam_args)\n",
    "gamma = 0.3  # final learning rate will be gamma * initial_lr\n",
    "lrd = gamma ** (1 / NUM_EPOCHS)\n",
    "optimizer = pyro.optim.ClippedAdam({'lr': LEARNING_RATE, 'lrd': lrd})\n",
    "\n",
    "# setup the inference algorithm\n",
    "svi = SVI(q10.model, q10.guide, optimizer, loss=TraceMeanField_ELBO(num_particles=10))\n",
    "\n",
    "train_elbo = []\n",
    "test_elbo = []\n",
    "# training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader, use_cuda=USE_CUDA)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
    "    for name in ['q_mean', 'q_sd']:\n",
    "        print(name, pyro.param(name).data.cpu().numpy())\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        # report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader, use_cuda=USE_CUDA)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n",
    "        print(\"[epoch %03d] average test loss: %.4f\" % (epoch, total_epoch_loss_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "greenhouse-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 2))\n",
    "plt.plot(train_elbo)\n",
    "plt.plot(test_elbo)\n",
    "plt.xlabel(\"SVI step\")\n",
    "plt.ylabel(\"ELBO loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "amazing-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_mean [1.5558014]\n",
      "q_sd [0.08776836]\n",
      "Rb$$$fc1.weight [[ 0.2591441  -0.75150406]\n",
      " [-0.30751866 -0.31916696]\n",
      " [ 0.47534242 -0.24159189]\n",
      " [ 0.15026985 -0.2605396 ]\n",
      " [ 0.547275    0.11498047]\n",
      " [-0.17844346 -0.24575871]\n",
      " [-0.45601827 -0.42156008]\n",
      " [ 0.4091637  -0.30653   ]\n",
      " [ 0.5341125   0.32771152]\n",
      " [-0.26084396 -0.47371638]]\n",
      "Rb$$$fc1.bias [ 0.16368476  0.40780124 -0.4231925   0.6072353  -0.08411942  0.34966308\n",
      " -0.0153637   0.7146586  -0.842425    0.17748408]\n",
      "Rb$$$fc2.weight [[-2.0590447e-01 -3.0602366e-01  1.6376610e-01 -3.5399836e-02\n",
      "  -2.1438444e-01 -4.4230863e-01  4.0226117e-02 -1.2070785e-01\n",
      "   1.3961646e-01  6.0127284e-02]\n",
      " [-2.8581223e-01 -2.1847406e-01 -3.7603220e-01 -2.8289577e-01\n",
      "  -4.6445319e-01  2.7336273e-03 -1.7911690e-01 -8.5003026e-02\n",
      "  -3.9996203e-02 -5.2213177e-02]\n",
      " [ 1.2996948e-01  2.3146282e-01  1.8517576e-01  2.0143108e-01\n",
      "  -6.1853927e-01  2.8070903e-01 -1.3928051e-01 -9.4754323e-02\n",
      "  -3.6504000e-02 -1.5747446e-01]\n",
      " [ 4.3821968e-02  2.5377011e-01 -1.5723646e-01  2.1299705e-01\n",
      "  -1.7889100e-01  1.5739106e-01  7.5469725e-02  5.5795699e-01\n",
      "  -5.3926259e-02  2.1016937e-01]\n",
      " [-3.8870946e-03 -1.2933023e-01  3.5606453e-01  4.6104673e-01\n",
      "  -9.5851589e-03 -4.0699281e-03  1.3564572e-01  4.2709643e-01\n",
      "   1.9746667e-02  1.1552675e-01]\n",
      " [-3.6151305e-01  1.3178222e-01 -1.7457968e-01 -4.1638833e-01\n",
      "  -2.0398133e-01  4.5660313e-02  1.2370794e-01  1.6528170e-01\n",
      "   2.6657397e-01 -2.8955314e-01]\n",
      " [-1.3450554e-01 -2.3411706e-01  3.1101641e-01 -2.9268166e-01\n",
      "  -1.3245432e-02  1.0960077e-01 -2.8155228e-01 -2.4236558e-01\n",
      "   1.2132958e-01 -2.8716725e-01]\n",
      " [ 4.2606795e-01  1.5669294e-01  1.4659047e-01  5.3352642e-01\n",
      "  -1.1333486e-01  6.5089501e-02  1.7593423e-01  4.2815956e-01\n",
      "  -7.1205948e-03  6.9283366e-02]\n",
      " [ 4.1068840e-01  1.9814233e-01  2.8751150e-01  1.9877590e-01\n",
      "   9.0233102e-02  4.7545794e-01  8.5937202e-02  1.3783343e-01\n",
      "  -1.8645869e-01  2.9435437e-02]\n",
      " [ 4.0603766e-01 -3.1977706e-04  1.5014853e-01  1.0092856e-01\n",
      "  -3.4123078e-01  3.6825445e-01  3.3777231e-01  1.5125979e-02\n",
      "  -2.3132260e-01  4.3768752e-01]]\n",
      "Rb$$$fc2.bias [-0.09377795  0.00414275  0.05438554  0.7196346   0.5419894  -0.33629796\n",
      "  0.05200757  0.70508176  0.37581667  0.27922472]\n",
      "Rb$$$fc3.weight [[ 0.02096099  0.00369605  0.0816631   0.5419756   0.33749145 -0.07131362\n",
      "  -0.24993193  0.5660071   0.29864153  0.17491041]]\n",
      "Rb$$$fc3.bias [0.48590985]\n"
     ]
    }
   ],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name).data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-comparative",
   "metadata": {},
   "source": [
    "Now we also model the network probabilistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "biblical-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q10nn_FBNN(PyroModule):\n",
    "    def __init__(self, y_mean, y_sd, include_TA=False):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.mode = 'generate'\n",
    "        input_dim = 2\n",
    "        #hidden_layer = 2\n",
    "        hidden_nodes = 8\n",
    "        self.y_mean = y_mean\n",
    "        self.y_sd = y_sd\n",
    "        self.include_TA = include_TA\n",
    "        if include_TA:\n",
    "            input_dim += 1\n",
    "        \n",
    "        self.fc1 = PyroModule[nn.Linear](input_dim, hidden_nodes)\n",
    "        self.fc1.weight = PyroSample(dist.Normal(0., 1.).expand([hidden_nodes, input_dim]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(0., 1.).expand([hidden_nodes]).to_event(1))\n",
    "        \n",
    "        self.fc2 = PyroModule[nn.Linear](hidden_nodes, hidden_nodes)\n",
    "        self.fc2.weight = PyroSample(dist.Normal(0., 1.).expand([hidden_nodes, hidden_nodes]).to_event(2))\n",
    "        self.fc2.bias = PyroSample(dist.Normal(0., 1.).expand([hidden_nodes]).to_event(1))\n",
    "    \n",
    "        self.fc3 = PyroModule[nn.Linear](hidden_nodes, 1)\n",
    "        self.fc3.weight = PyroSample(dist.Normal(0., 1.).expand([1, hidden_nodes]).to_event(2))\n",
    "        self.fc3.bias = PyroSample(dist.Normal(0., 1.).expand([1]).to_event(1))\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        if self.include_TA:\n",
    "            x_1 = self.relu(self.fc1(x))\n",
    "            x_2 = self.relu(self.fc2(x_1))\n",
    "            rb = self.fc3(x_2).squeeze()            \n",
    "            sigma = pyro.sample(\"sigma\", dist.Uniform(0., 1.))\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                q10_loc = x.new_zeros(torch.Size((x.shape[0], 1)))\n",
    "                q10_scale = x.new_ones(torch.Size((x.shape[0], 1)))\n",
    "                q10 = pyro.sample(\"Q10\", dist.Normal(q10_loc, q10_scale).to_event(1))\n",
    "                mu = rb * torch.pow(q10, 0.1 * (x[:,-1]-15))\n",
    "                mu = (mu - self.y_mean) / self.y_sd\n",
    "                obs = pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "        else:\n",
    "            x_1 = self.relu(self.fc1(x[:,:-1]))\n",
    "            x_2 = self.relu(self.fc2(x_1))\n",
    "            rb = self.fc3(x_2)      \n",
    "            sigma = pyro.sample(\"sigma\", dist.Uniform(0., 1.))\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                q10_loc = x.new_zeros(torch.Size((x.shape[0], 1)))\n",
    "                q10_scale = x.new_ones(torch.Size((x.shape[0], 1)))\n",
    "                q10 = pyro.sample(\"Q10\", dist.Normal(q10_loc, q10_scale).to_event(1))\n",
    "                print(rb)\n",
    "                print(q10)\n",
    "                print(x[:,-1])\n",
    "                mu = torch.multiply(rb, torch.pow(q10, 0.1 * (x[:,-1].unsqueeze(-1)-15)))\n",
    "                print(mu)\n",
    "                mu = (mu - self.y_mean) / self.y_sd\n",
    "                obs = pyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = q10nn_FBNN(Y_mean, Y_sd)\n",
    "guide = AutoDiagonalNormal(model)\n",
    "adam = pyro.optim.Adam({\"lr\": 1e-5})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
    "\n",
    "pyro.clear_param_store()\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    L2_accum = 0.0\n",
    "    for i, traindata in enumerate(trainLoader, 0):\n",
    "        inputs_curr, targets_curr = traindata\n",
    "        \n",
    "        loss = svi.step(inputs_curr, targets_curr)\n",
    "        L2_accum += loss.item()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "arctic-trunk",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(-0.0453).pow((0.1 * ( 9.4200 - 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "organic-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_TA = True\n",
    "Q10_init = 2.0\n",
    "model = q10nn(Y_mean, Y_sd, Q10_init, include_TA).to(device)\n",
    "model.apply(weights_init)\n",
    "lr_init = 0.01\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam([{'params': model.lls.parameters(), 'lr': lr_init,'weight_decay': weight_decay},\n",
    "                                {'params': model.Q, 'lr': lr_init, 'weight_decay': 0}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-memorabilia",
   "metadata": {},
   "source": [
    "# Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "frozen-documentary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, L2 train: 1.62852, L1 vali: 1.50394, Q: 2.00000\n",
      "Training from scratch\n",
      "Epoch: 1, L2 train: 0.16138, L1 vali: 0.00883, Q: 1.62825\n",
      "Epoch: 2, L2 train: 0.00427, L1 vali: 0.00413, Q: 1.59152\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a755e0a8d81b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mlossL2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterionL2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mlossL2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_L2 = []\n",
    "history_L2val = []\n",
    "history_Q = []\n",
    "EPOCHS = 10\n",
    "model.to('cpu')\n",
    "\n",
    "criterionL2 = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Compute the initial Validation and Training loss\n",
    "model.eval()\n",
    "L2_accum = 0.0\n",
    "for i, traindata in enumerate(trainLoader, 0):\n",
    "    inputs_curr, targets_curr = traindata\n",
    "\n",
    "    outputs= model(inputs_curr)\n",
    "    outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "    lossL2 = criterionL2(outputs, targets_curr)\n",
    "    L2_accum += lossL2.item()\n",
    "\n",
    "L2val_accum = 0.0\n",
    "for i, validata in enumerate(valLoader, 0):\n",
    "    inputs_curr, targets_curr = validata\n",
    "\n",
    "    outputs= model(inputs_curr)\n",
    "    outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "    lossL2val = criterionL2(outputs, targets_curr)\n",
    "    L2val_accum += lossL2val.item()\n",
    "\n",
    "history_Q.append(model.Q.clone())\n",
    "history_L2.append( L2_accum / len(trainLoader) )\n",
    "history_L2val.append( L2val_accum / len(valLoader) )\n",
    "\n",
    "print( \"Epoch: {}, L2 train: {:7.5f}, L1 vali: {:7.5f}, Q: {:7.5f}\".format(0, history_L2[-1], history_L2val[-1], history_Q[-1]) )\n",
    "\n",
    "# Now start the training process\n",
    "print(\"Training from scratch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    L2_accum = 0.0\n",
    "    for i, traindata in enumerate(trainLoader, 0):\n",
    "        inputs_curr, targets_curr = traindata\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        gen_out = model(inputs_curr)\n",
    "        \n",
    "        lossL2 = criterionL2(gen_out, targets_curr)\n",
    "        lossL2.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        L2_accum += lossL2.item()        \n",
    "\n",
    "  # validation\n",
    "    model.eval()\n",
    "    L2val_accum = 0.0\n",
    "    for i, validata in enumerate(valLoader, 0):\n",
    "        inputs_curr, targets_curr = validata\n",
    "        \n",
    "        outputs= model(inputs_curr)\n",
    "        outputs_curr = outputs.data.cpu().numpy()\n",
    "\n",
    "        lossL2val = criterionL2(outputs, targets_curr)\n",
    "        L2val_accum += lossL2val.item()\n",
    "    \n",
    "    history_Q.append(model.Q.clone())\n",
    "    \n",
    "    # data for graph plotting\n",
    "    history_L2.append( L2_accum / len(trainLoader) )\n",
    "    history_L2val.append( L2val_accum / len(valLoader) )\n",
    "\n",
    "    if epoch % 1 ==0:\n",
    "        print( \"Epoch: {}, L2 train: {:7.5f}, L1 vali: {:7.5f}, Q: {:7.5f}\".format(epoch+1, history_L2[-1], history_L2val[-1], history_Q[-1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-touch",
   "metadata": {},
   "source": [
    "# Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l2train = np.asarray(history_L2)\n",
    "l2vali  = np.asarray(history_L2val)\n",
    "Q = np.asarray(history_Q)\n",
    "\n",
    "fix, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q, 'b', label = 'Q10')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Q10')\n",
    "ax[0].set_title('Q10 training history')\n",
    "\n",
    "ax[1].plot(np.arange(l2train.shape[0]),l2train,'b',label='Training loss')\n",
    "ax[1].plot(np.arange(l2vali.shape[0] ),l2vali ,'g',label='Validation loss')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('MSE')\n",
    "ax[1].set_title('MSE training history')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-jungle",
   "metadata": {},
   "source": [
    "# Nondeterministic Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "promising-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class q10nn_VB(nn.Module):\n",
    "    def __init__(self, y_mean, y_sd, Q10_init, std_init=0.1, include_TA=True):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.mode = 'generate'\n",
    "        input_dim = 2\n",
    "        hidden_layer = 2\n",
    "        hidden_nodes = 8\n",
    "        if include_TA:\n",
    "            input_dim += 1\n",
    "\n",
    "        self.include_TA=include_TA\n",
    "        self.y_mean = y_mean\n",
    "        self.y_sd = y_sd\n",
    "        \n",
    "        self.mu = torch.nn.Parameter(torch.tensor(float(Q10_init)))\n",
    "        self.mu.requires_grad = True\n",
    "        self.logvar = torch.nn.Parameter(torch.tensor(np.log(std_init**2)))\n",
    "        self.logvar.requires_grad = True\n",
    "        \n",
    "        if hidden_layer == 0:\n",
    "            layers = [nn.Linear(input_dim, 1)]\n",
    "            layers.append(nn.Softplus())\n",
    "        else:\n",
    "            layers = [nn.Linear(input_dim, hidden_nodes)]\n",
    "            layers.append(nn.ReLU())\n",
    "            \n",
    "            for i in range(hidden_layer-1):\n",
    "                layers.append(nn.Linear(hidden_nodes, hidden_nodes))\n",
    "                layers.append(nn.ReLU())\n",
    "            \n",
    "            layers.append(nn.Linear(hidden_nodes, 1))\n",
    "            layers.append(nn.Softplus())\n",
    "                                                            \n",
    "        self.lls = nn.Sequential(*layers)\n",
    "    \n",
    "    def sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # Reparameterization\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.sample(self.mu, self.logvar)\n",
    "        if self.include_TA:\n",
    "            y = self.lls(x).squeeze(-1) * torch.pow(q, 0.1 * (x[:,-1]-15))\n",
    "        else:\n",
    "            y = self.lls(x[:,:-1]).squeeze(-1) * torch.pow(q, 0.1 * (x[:,-1]-15))\n",
    "        y = (y - self.y_mean) / self.y_sd\n",
    "        return y\n",
    "    \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "floating-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0.1\n",
    "Q10_init = 2.0\n",
    "Q10_init = 2.0\n",
    "\n",
    "std_init = 0.5\n",
    "Q10_prior = Q10_init\n",
    "logvar_prior = torch.tensor(np.log(std_init**2))\n",
    "\n",
    "def VB_loss(y_pred, y, mu, logvar):\n",
    "    # Reconstruction losses are summed over all elements and batch, normalize loss by batch size\n",
    "    mse_loss = F.mse_loss(y_pred, y)    \n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - logvar_prior - ((mu-Q10_prior).pow(2) + logvar.exp())/logvar_prior.exp())\n",
    "    total_loss = mse_loss + beta * kld_loss\n",
    "\n",
    "    return total_loss, mse_loss, kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-joyce",
   "metadata": {},
   "source": [
    "# Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-activity",
   "metadata": {},
   "source": [
    "Note that we choose a learning rate for Q10 that is 10 times higher than the one of the other parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "corrected-chancellor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(2., device='cuda:0', requires_grad=True)\n",
      "tensor(0.5000, device='cuda:0', dtype=torch.float64, grad_fn=<ExpBackward>)\n"
     ]
    }
   ],
   "source": [
    "include_TA = True\n",
    "model = q10nn_VB(Y_mean, Y_sd, Q10_init, std_init, include_TA).to(device)\n",
    "model.apply(weights_init)\n",
    "print(model.mu)\n",
    "print(model.logvar.mul(0.5).exp())\n",
    "lr_init = 0.01\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.Adam([{'params': model.lls.parameters(), 'lr': lr_init,'weight_decay': weight_decay},\n",
    "                                {'params': [model.mu, model.logvar], 'lr': lr_init*10, 'weight_decay': 0}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "worse-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, total train: 1.75868, mse train: 7.06058, KLD train: 0.00000, total test: 0.40400, mse test: 1.62193, KLD test: 0.00000\n",
      "Q mean:  2.00, Q std:  0.50\n",
      "Training from scratch\n",
      "Epoch: 1, total train: 0.26875, mse train: 0.88852, KLD train: 0.47434, total test: 0.00807, mse test: 0.01930, KLD test: 0.03263\n",
      "Q mean:  2.0286, Q std:  0.3323\n",
      "Epoch: 2, total train: 0.07201, mse train: 0.21482, KLD train: 0.18498, total test: 0.01166, mse test: 0.02472, KLD test: 0.05505\n",
      "Q mean:  2.1179, Q std:  0.2989\n",
      "Epoch: 3, total train: 0.06737, mse train: 0.17419, KLD train: 0.23986, total test: 0.00835, mse test: 0.01480, KLD test: 0.04666\n",
      "Q mean:  2.1584, Q std:  0.3278\n",
      "Epoch: 4, total train: 0.07011, mse train: 0.18346, KLD train: 0.24415, total test: 0.00550, mse test: 0.00558, KLD test: 0.04112\n",
      "Q mean:  2.0404, Q std:  0.3143\n",
      "Epoch: 5, total train: 0.06701, mse train: 0.17740, KLD train: 0.22819, total test: 0.00872, mse test: 0.01058, KLD test: 0.06087\n",
      "Q mean:  1.9736, Q std:  0.2774\n",
      "Epoch: 6, total train: 0.07167, mse train: 0.19515, KLD train: 0.23062, total test: 0.00820, mse test: 0.00478, KLD test: 0.07013\n",
      "Q mean:  2.0160, Q std:  0.2629\n",
      "Epoch: 7, total train: 0.07317, mse train: 0.19120, KLD train: 0.25550, total test: 0.00503, mse test: 0.00513, KLD test: 0.03752\n",
      "Q mean:  2.0882, Q std:  0.3290\n",
      "Epoch: 8, total train: 0.06838, mse train: 0.17580, KLD train: 0.24596, total test: 0.01114, mse test: 0.00672, KLD test: 0.09463\n",
      "Q mean:  2.0763, Q std:  0.2342\n",
      "Epoch: 9, total train: 0.06404, mse train: 0.16283, KLD train: 0.23483, total test: 0.00436, mse test: 0.00575, KLD test: 0.02931\n",
      "Q mean:  2.1047, Q std:  0.3541\n",
      "Epoch: 10, total train: 0.07869, mse train: 0.19963, KLD train: 0.28965, total test: 0.02172, mse test: 0.03394, KLD test: 0.13268\n",
      "Q mean:  1.9029, Q std:  0.1959\n"
     ]
    }
   ],
   "source": [
    "history_total = []\n",
    "history_mse = []\n",
    "history_kld = []\n",
    "\n",
    "history_total_val = []\n",
    "history_mse_val = []\n",
    "history_kld_val = []\n",
    "\n",
    "history_Q = []\n",
    "history_std = []\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# Compute the initial Validation and Training loss\n",
    "model.eval()\n",
    "total_accum = 0.0\n",
    "mse_accum = 0.0\n",
    "kld_accum = 0.0\n",
    "\n",
    "for i, traindata in enumerate(trainLoader, 0):\n",
    "    inputs_curr, targets_curr = traindata\n",
    "\n",
    "    outputs= model(inputs_curr.to(device))\n",
    "    total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "\n",
    "    total_accum += total_loss.item()\n",
    "    mse_accum += mse_loss.item()\n",
    "    kld_accum += kld_loss.item()\n",
    "    \n",
    "total_accum_val = 0.0\n",
    "mse_accum_val = 0.0\n",
    "kld_accum_val = 0.0\n",
    "\n",
    "for i, validata in enumerate(valLoader, 0):\n",
    "    inputs_curr, targets_curr = validata\n",
    "\n",
    "    outputs= model(inputs_curr.to(device))\n",
    "    \n",
    "    total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "    \n",
    "    total_accum_val += total_loss.item()\n",
    "    mse_accum_val += mse_loss.item()\n",
    "    kld_accum_val += kld_loss.item()\n",
    "\n",
    "history_Q.append(model.mu.clone())\n",
    "history_std.append(model.logvar.clone())\n",
    "\n",
    "history_total.append( total_accum / len(trainLoader) )\n",
    "history_mse.append( mse_accum / len(valLoader) )\n",
    "history_kld.append( kld_accum / len(trainLoader) )\n",
    "\n",
    "history_total_val.append( total_accum_val / len(trainLoader) )\n",
    "history_mse_val.append( mse_accum_val / len(valLoader) )\n",
    "history_kld_val.append( kld_accum_val / len(trainLoader) )\n",
    "\n",
    "print( \"Epoch: {}, total train: {:7.5f}, mse train: {:7.5f}, KLD train: {:7.5f}, total test: {:7.5f}, mse test: {:7.5f}, KLD test: {:7.5f}\".format(0, history_total[-1], history_mse[-1], history_kld[-1], history_total_val[-1], history_mse_val[-1], history_kld_val[-1]))\n",
    "print( f\"Q mean: {history_Q[-1]: .2f}, Q std: {torch.exp(1/2*history_std[-1]): .2f}\")\n",
    "\n",
    "# Now start the training process\n",
    "print(\"Training from scratch\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    total_accum = 0.0\n",
    "    mse_accum = 0.0\n",
    "    kld_accum = 0.0\n",
    "\n",
    "    for i, traindata in enumerate(trainLoader, 0):\n",
    "        inputs_curr, targets_curr = traindata\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs_curr.to(device))\n",
    "        total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)    \n",
    "        total_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        total_accum += total_loss.item()\n",
    "        mse_accum += mse_loss.item()\n",
    "        kld_accum += kld_loss.item()\n",
    "\n",
    "  # validation\n",
    "    model.eval()\n",
    "    total_accum_val = 0.0\n",
    "    mse_accum_val = 0.0\n",
    "    kld_accum_val = 0.0\n",
    "\n",
    "    for i, validata in enumerate(valLoader, 0):\n",
    "        inputs_curr, targets_curr = validata\n",
    "        \n",
    "        outputs= model(inputs_curr.to(device))\n",
    "        total_loss, mse_loss, kld_loss = VB_loss(outputs, targets_curr.to(device), model.mu, model.logvar)\n",
    "\n",
    "        total_accum_val += total_loss.item()\n",
    "        mse_accum_val += mse_loss.item()\n",
    "        kld_accum_val += kld_loss.item()\n",
    "            \n",
    "   \n",
    "    history_Q.append(model.mu.clone())\n",
    "    history_std.append(model.logvar.clone())\n",
    "\n",
    "    history_total.append( total_accum / len(trainLoader) )\n",
    "    history_mse.append( mse_accum / len(valLoader) )\n",
    "    history_kld.append( kld_accum / len(trainLoader) )\n",
    "\n",
    "    history_total_val.append( total_accum_val / len(trainLoader) )\n",
    "    history_mse_val.append( mse_accum_val / len(valLoader) )\n",
    "    history_kld_val.append( kld_accum_val / len(trainLoader) )\n",
    "\n",
    "    if epoch % 1 ==0:\n",
    "        print( \"Epoch: {}, total train: {:7.5f}, mse train: {:7.5f}, KLD train: {:7.5f}, total test: {:7.5f}, mse test: {:7.5f}, KLD test: {:7.5f}\".format(epoch+1, history_total[-1], history_mse[-1], history_kld[-1], history_total_val[-1], history_mse_val[-1], history_kld_val[-1]))\n",
    "        print( f\"Q mean: {history_Q[-1]: .4f}, Q std: {torch.exp(1/2*history_std[-1]): .4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-moderator",
   "metadata": {},
   "source": [
    "# Plot the training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "totaltrain = np.asarray(history_total)\n",
    "totalvali  = np.asarray(history_total_val)\n",
    "msetrain = np.asarray(history_mse)\n",
    "msevali  = np.asarray(history_mse_val)\n",
    "kldtrain = np.asarray(history_kld)\n",
    "kldvali  = np.asarray(history_kld_val)\n",
    "Q = np.asarray(history_Q)\n",
    "std = np.asarray(history_std)\n",
    "\n",
    "fix, ax = plt.subplots(1,5, figsize = (25,5))\n",
    "\n",
    "ax[0].plot(np.arange(Q.shape[0]), Q, 'b', label = 'Q10')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Q10')\n",
    "ax[0].set_title('Q10 training history')\n",
    "\n",
    "ax[1].plot(np.arange(std.shape[0]), np.exp(1/2*std), 'b', label = 'std')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('std')\n",
    "ax[1].set_title('std training history')\n",
    "\n",
    "ax[2].plot(np.arange(totaltrain.shape[0]),totaltrain,'b',label='Training loss')\n",
    "ax[2].plot(np.arange(totalvali.shape[0] ),totalvali ,'g',label='Validation loss')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('Total loss')\n",
    "ax[2].set_title('Total loss training history')\n",
    "\n",
    "ax[3].plot(np.arange(msetrain.shape[0]),msetrain,'b',label='Training loss')\n",
    "ax[3].plot(np.arange(msevali.shape[0] ),msevali ,'g',label='Validation loss')\n",
    "ax[3].set_xlabel('Epoch')\n",
    "ax[3].set_ylabel('MSE loss')\n",
    "ax[3].set_title('MSE training history')\n",
    "\n",
    "ax[4].plot(np.arange(kldtrain.shape[0]),kldtrain,'b',label='Training loss')\n",
    "ax[4].plot(np.arange(kldvali.shape[0] ),kldvali ,'g',label='Validation loss')\n",
    "ax[4].set_xlabel('Epoch')\n",
    "ax[4].set_ylabel('kld loss')\n",
    "ax[4].set_title('kld training history')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-beverage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
